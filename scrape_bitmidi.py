import requests
import os
import time
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from tqdm import tqdm

# ==============================================================================
# 1. CONFIGURATION (Inchang√©e)
# ==============================================================================

SAVE_DIR = "bitmidi_dataset"
BASE_URL = "https://bitmidi.com"
SITEMAP_INDEX_URL = "https://bitmidi.com/sitemap.xml" # Renomm√© pour plus de clart√©
DELAY_SECONDS = 2
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# ==============================================================================
# 2. LE SCRIPT DE SCRAPING (MIS √Ä JOUR)
# ==============================================================================

def scrape_bitmidi():
    print("üöÄ D√©marrage du scraping de bitmidi.com...")
    print(f"Les fichiers seront sauvegard√©s dans le dossier : '{SAVE_DIR}'")
    os.makedirs(SAVE_DIR, exist_ok=True)
    
    # --- √âtape 1: G√©rer l'index de sitemaps ---
    print(f"\n1. T√©l√©chargement de l'index de sitemaps depuis {SITEMAP_INDEX_URL}...")
    try:
        response = requests.get(SITEMAP_INDEX_URL, headers=HEADERS)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå ERREUR : Impossible de t√©l√©charger l'index de sitemaps. Erreur : {e}")
        return

    # Analyser l'index pour trouver les URLs des VRAIES sitemaps
    root = ET.fromstring(response.content)
    ns = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
    
    # Le chemin a chang√© : on cherche <sitemap> puis <loc>
    sitemap_urls = [elem.text for elem in root.findall('sitemap:sitemap/sitemap:loc', ns)]
    
    if not sitemap_urls:
        print("‚ùå ERREUR : Aucune sitemap trouv√©e dans l'index. Le site a peut-√™tre encore chang√© de structure.")
        return
        
    print(f"‚úÖ {len(sitemap_urls)} sitemap(s) trouv√©(s) dans l'index. Analyse de chaque sitemap...")
    
    # --- √âtape 1.5: Parcourir chaque sitemap pour collecter TOUTES les URLs ---
    all_song_urls = []
    for sitemap_url in tqdm(sitemap_urls, desc="Analyse des sitemaps"):
        try:
            time.sleep(1) # Petit d√©lai
            sitemap_response = requests.get(sitemap_url, headers=HEADERS)
            sitemap_response.raise_for_status()
            
            sitemap_root = ET.fromstring(sitemap_response.content)
            # Ici on utilise l'ancien chemin pour trouver les URLs des chansons
            urls_from_this_map = [elem.text for elem in sitemap_root.findall('sitemap:url/sitemap:loc', ns)]
            all_song_urls.extend(urls_from_this_map)
            
        except requests.exceptions.RequestException as e:
            tqdm.write(f"‚ö†Ô∏è Impossible de traiter la sitemap {sitemap_url}: {e}")

    # Filtrer la liste compl√®te pour ne garder que les pages de chansons
    midi_page_urls = [url for url in all_song_urls if url.endswith('-mid')]
    
    if not midi_page_urls:
        print("‚ùå ERREUR : Aucune URL de chanson trouv√©e apr√®s avoir analys√© toutes les sitemaps.")
        return
        
    print(f"‚úÖ Au total, {len(midi_page_urls)} pages de chansons ont √©t√© collect√©es.")
    
    # --- √âtape 2: D√©marrer le t√©l√©chargement des fichiers (inchang√©e) ---
    print("\n2. D√©marrage du t√©l√©chargement des fichiers MIDI (cela peut prendre plusieurs heures)...")
    
    for url in tqdm(midi_page_urls, desc="T√©l√©chargement", unit="fichier"):
        try:
            time.sleep(DELAY_SECONDS)
            page_response = requests.get(url, headers=HEADERS)
            page_response.raise_for_status()
            
            soup = BeautifulSoup(page_response.content, 'html.parser')
            download_link_tag = soup.find('a', attrs={'download': True})
            
            if download_link_tag and 'href' in download_link_tag.attrs:
                relative_link = download_link_tag['href']
                download_url = BASE_URL + relative_link
                filename = download_link_tag['download']
                if not filename.lower().endswith('.mid'):
                    filename += '.mid'
                save_path = os.path.join(SAVE_DIR, filename)

                if not os.path.exists(save_path):
                    midi_response = requests.get(download_url, headers=HEADERS)
                    midi_response.raise_for_status()
                    with open(save_path, 'wb') as f:
                        f.write(midi_response.content)
                        
        except requests.exceptions.RequestException as e:
            tqdm.write(f"‚ö†Ô∏è Erreur de r√©seau pour {url}: {e}")
        except Exception as e:
            tqdm.write(f"‚ö†Ô∏è Erreur inattendue pour {url}: {e}")

    print("\nüéâ Scraping termin√© !")
    print(f"Tous les fichiers disponibles ont √©t√© t√©l√©charg√©s dans le dossier '{SAVE_DIR}'.")

if __name__ == '__main__':
    scrape_bitmidi()